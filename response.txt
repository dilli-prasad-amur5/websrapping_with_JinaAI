[1] Title: Retrieval Augmented Generation
[1] URL Source: https://www.databricks.com/glossary/retrieval-augmented-generation-rag
[1] Description: Retrieval augmented generation or <strong>RAG</strong> is an architectural approach that pulls your <strong>data</strong> as context for large language models (LLMs) to improve relevancy.
[1] Published Time: Wed, 10/18/2023 - 15:21
[1] Markdown Content:
What is Retrieval Augmented Generation (RAG)? | Databricks
===============

  

![Image 1: bg-landing-tablet-74ec2ad32b2fe1f43aecf674d3f3bef0](https://www.databricks.com/glossaries-assets/static/bg-landing-tablet-74ec2ad32b2fe1f43aecf674d3f3bef0.png)

[Skip to main content](https://www.databricks.com/glossary/retrieval-augmented-generation-rag#main)

[![Image 2](blob:https://www.databricks.com/c39621b393e59e244d5b334ce7a6a7c0)](https://www.databricks.com/)

[Login](https://accounts.cloud.databricks.com/)

[![Image 3](blob:https://www.databricks.com/c39621b393e59e244d5b334ce7a6a7c0)](https://www.databricks.com/)

*   Why Databricks
    
    *   *   Discover
            
            *   [For Executives](https://www.databricks.com/why-databricks/executives)
                
            *   [For Startups](https://www.databricks.com/product/startups)
                
            *   [Lakehouse Architecture](https://www.databricks.com/product/data-lakehouse)
                
            *   [DatabricksIQ](https://www.databricks.com/product/databricksiq)
                
            *   [Mosaic Research](https://www.databricks.com/research/mosaic)
                
            
        *   Customers
            
            *   [Featured Stories](https://www.databricks.com/customers)
                
            *   [See All Customers](https://www.databricks.com/customers/all)
                
            
        *   Partners
            
            *   [Cloud Providers Databricks on AWS, Azure, and GCP](https://www.databricks.com/company/partners/cloud-partners)
                
            *   [Consulting & System Integrators Experts to build, deploy and migrate to Databricks](https://www.databricks.com/company/partners/consulting-and-si)
                
            *   [Technology Partners Connect your existing tools to your Lakehouse](https://www.databricks.com/company/partners/technology-partner-program)
                
            *   [C&SI Partner Program Build, deploy or migrate to the Lakehouse](https://www.databricks.com/company/partners/consulting-and-si/candsi-partner-program)
                
            *   [Data Partners Access the ecosystem of data consumers](https://www.databricks.com/company/partners/data-partner-program)
                
            *   [Partner Solutions Find custom industry and migration solutions](https://www.databricks.com/company/partners/consulting-and-si/partner-solutions)
                
            *   [Built on Databricks Build, market and grow your business](https://www.databricks.com/company/partners/built-on-partner-program)
                
            
    
*   Product
    
    *   *   Databricks Platform
            
            *   [Platform Overview A unified platform for data, analytics and AI](https://www.databricks.com/product/data-intelligence-platform)
                
            *   [Data Management Data reliability, security and performance](https://www.databricks.com/product/delta-lake-on-databricks)
                
            *   [Sharing An open, secure, zero-copy sharing for all data](https://www.databricks.com/product/delta-sharing)
                
            *   [Data Warehousing Serverless data warehouse for SQL analytics](https://www.databricks.com/product/databricks-sql)
                
            *   [Governance Unified governance for all data, analytics and AI assets](https://www.databricks.com/product/unity-catalog)
                
            *   [Real-Time Analytics Real-time analytics, AI and applications made simple](https://www.databricks.com/product/data-streaming)
                
            *   [Artificial Intelligence Build and deploy ML and GenAI applications](https://www.databricks.com/product/machine-learning)
                
            *   [Data Engineering ETL and orchestration for batch and streaming data](https://www.databricks.com/solutions/data-engineering)
                
            *   [Business Intelligence Intelligent analytics for real-world data](https://www.databricks.com/product/ai-bi)
                
            *   [Data Science Collaborative data science at scale](https://www.databricks.com/product/data-science)
                
            
        *   Integrations and Data
            
            *   [Marketplace Open marketplace for data, analytics and AI](https://www.databricks.com/product/marketplace)
                
            *   [IDE Integrations Build on the Lakehouse in your favorite IDE](https://www.databricks.com/product/data-science/ide-integrations)
                
            *   [Partner Connect Discover and integrate with the Databricks ecosystem](https://www.databricks.com/partnerconnect)
                
            
        *   Pricing
            
            *   [Databricks Pricing Explore product pricing, DBUs and more](https://www.databricks.com/product/pricing)
                
            *   [Cost Calculator Estimate your compute costs on any cloud](https://www.databricks.com/product/pricing/product-pricing/instance-types)
                
            
        *   Open Source
            
            *   [Open Source Technologies Learn more about the innovations behind the platform](https://www.databricks.com/product/open-source)
                
            
    
*   Solutions
    
    *   *   Databricks for Industries
            
            *   [Communications](https://www.databricks.com/solutions/industries/communications)
                
            *   [Media and Entertainment](https://www.databricks.com/solutions/industries/media-and-entertainment)
                
            *   [Financial Services](https://www.databricks.com/solutions/industries/financial-services)
                
            *   [Public Sector](https://www.databricks.com/solutions/industries/public-sector)
                
            *   [Healthcare & Life Sciences](https://www.databricks.com/solutions/industries/healthcare-and-life-sciences)
                
            *   [Retail](https://www.databricks.com/solutions/industries/retail-industry-solutions)
                
            *   [Manufacturing](https://www.databricks.com/solutions/industries/manufacturing-industry-solutions)
                
            *   [See All Industries](https://www.databricks.com/solutions)
                
            
        *   Cross Industry Solutions
            
            *   [Customer Data Platform](https://www.databricks.com/solutions/industries/customer-experience)
                
            *   [Cyber Security](https://www.databricks.com/solutions/industries/cybersecurity)
                
            
        *   Migration & Deployment
            
            *   [Data Migration](https://www.databricks.com/solutions/migration)
                
            *   [Professional Services](https://www.databricks.com/professional-services)
                
            
        *   Solution Accelerators
            
            *   [Explore Accelerators Move faster toward outcomes that matter](https://www.databricks.com/solutions/accelerators)
                
            
    
*   Resources
    
    *   *   Training and Certification
            
            *   [Learning Overview Hub for training, certification, events and more](https://www.databricks.com/learn)
                
            *   [Training Overview Discover curriculum tailored to your needs](https://www.databricks.com/learn/training/home)
                
            *   [Databricks Academy Sign in to the Databricks learning platform](https://www.databricks.com/learn/training/login)
                
            *   [Certification Gain recognition and differentiation](https://www.databricks.com/learn/training/certification)
                
            *   [University Alliance Want to teach Databricks? See how.](https://www.databricks.com/university)
                
            
        *   Events
            
            *   [Data + AI Summit](https://www.databricks.com/dataaisummit)
                
            *   [Data + AI World Tour](https://www.databricks.com/dataaisummit/worldtour)
                
            *   [Data Intelligence Days](https://www.databricks.com/lp/data-intelligence-days)
                
            *   [Event Calendar](https://www.databricks.com/events)
                
            
        *   Blog and Podcasts
            
            *   [Databricks Blog Explore news, product announcements, and more](https://www.databricks.com/blog)
                
            *   [Databricks Mosaic Research Blog Discover the latest in our Gen AI research](https://www.databricks.com/blog/category/generative-ai/mosaic-research)
                
            *   [Data Brew Podcast Let’s talk data!](https://www.databricks.com/discover/data-brew)
                
            *   [Champions of Data + AI Podcast Insights from data leaders powering innovation](https://www.databricks.com/discover/champions-of-data-and-ai)
                
            
        *   Get Help
            
            *   [Customer Support](https://www.databricks.com/support)
                
            *   [Documentation](https://docs.databricks.com/en/index.html)
                
            *   [Community](https://community.databricks.com/s/)
                
            
        *   Dive Deep
            
            *   [Resource Center](https://www.databricks.com/resources)
                
            *   [Demo Center](https://www.databricks.com/resources/demos)
                
            
    
*   About
    
    *   *   Company
            
            *   [Who We Are](https://www.databricks.com/company/about-us)
                
            *   [Our Team](https://www.databricks.com/company/leadership-team)
                
            *   [Databricks Ventures](https://www.databricks.com/databricks-ventures)
                
            *   [Contact Us](https://www.databricks.com/company/contact)
                
            
        *   Careers
            
            *   [Working at Databricks](https://www.databricks.com/company/careers)
                
            *   [Open Jobs](https://www.databricks.com/company/careers/open-positions)
                
            
        *   Press
            
            *   [Awards and Recognition](https://www.databricks.com/company/awards-and-recognition)
                
            *   [Newsroom](https://www.databricks.com/company/newsroom)
                
            
        *   Security and Trust
            
            *   [Security and Trust](https://www.databricks.com/trust)
                
            
    

*   Ready to get started?
    
*   [Get a Demo](https://www.databricks.com/resources/demos)

*   [Login](https://accounts.cloud.databricks.com/)
*   [Contact Us](https://www.databricks.com/company/contact)
*   [Try Databricks](https://www.databricks.com/try-databricks)

Retrieval Augmented Generation
==============================

1.  [All](https://www.databricks.com/glossary)
2.  /
    
    Retrieval Augmented Generation
    

What Is Retrieval Augmented Generation, or RAG?
-----------------------------------------------

Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of [large language model (LLM)](https://www.databricks.com/glossary/large-language-models-llm) applications by leveraging custom data. This is done by retrieving data/documents relevant to a question or task and providing them as context for the LLM. RAG has shown success in support chatbots and Q&A systems that need to maintain up-to-date information or access domain-specific knowledge.

Here’s more to explore

[![Image 4](https://www.databricks.com/glossaries-assets/static/02081441571ce26701340cbe01ef9e06/2022-06-Big-Book-of-MLOps-TY-TN-362x190-2x-1670616323.png)](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)


[2] Title: Challenges moving data science proof of concepts (POCs) to production
[2] URL Source: https://medium.com/analytics-and-data/challenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1
[2] Description: Many companies want to be able to leverage the power of <strong>data</strong> and look to invest in <strong>data</strong> <strong>science</strong> proof of concepts as a way to tiptoe into it. Unfortunately, a high number of proof of concept…
[2] Published Time: 2020-06-01T20:29:06.132Z
[2] Markdown Content:
Challenges moving data science proof of concepts (POCs) to production | by Julien Kervizic | Hacking Analytics | Medium
===============
 

[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F458d89b6a9a1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)

Sign up

[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------)

[](https://medium.com/?source=---two_column_layout_nav----------------------------------)

[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)

[](https://medium.com/search?source=---two_column_layout_nav----------------------------------)

Sign up

[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&source=post_page---two_column_layout_nav-----------------------global_nav-----------)

![Image 1](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)

Member-only story

Challenges moving data science proof of concepts (POCs) to production
=====================================================================

[![Image 2: Julien Kervizic](https://miro.medium.com/v2/resize:fill:88:88/0*8KNHDwr8y_G86xGi.)](https://medium.com/@julienkervizic?source=post_page-----458d89b6a9a1--------------------------------)[![Image 3: Hacking Analytics](https://miro.medium.com/v2/resize:fill:48:48/1*rwY9i9rSi9ySOlgw_ZO7yQ.png)](https://medium.com/analytics-and-data?source=post_page-----458d89b6a9a1--------------------------------)

[Julien Kervizic](https://medium.com/@julienkervizic?source=post_page-----458d89b6a9a1--------------------------------)

·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F62925548a091&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&user=Julien+Kervizic&userId=62925548a091&source=post_page-62925548a091----458d89b6a9a1---------------------post_header-----------)

Published in

[Hacking Analytics](https://medium.com/analytics-and-data?source=post_page-----458d89b6a9a1--------------------------------)

·

12 min read

·

Jun 1, 2020

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-and-data%2F458d89b6a9a1&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&user=Julien+Kervizic&userId=62925548a091&source=-----458d89b6a9a1---------------------clap_footer-----------)

\--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F458d89b6a9a1&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&source=-----458d89b6a9a1---------------------bookmark_footer-----------)

Share

![Image 4](https://medium.com/analytics-and-data/challenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1)

Photo by [Arshad Pooloo](https://unsplash.com/@ar_shad?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

Many companies want to be able to leverage the power of data and look to invest in data science proof of concepts as a way to tiptoe into it. Unfortunately, a high number of proof of concept initiatives fail to make it to production. From my experience, there are multiple reasons why this happens. The challenges with operating data science are more than purely about creating a predictive model out of sample data. There are organizational, project, data, and infrastructure that an organization must face along their journey to be data-driven.

Organizational Issues:
======================

![Image 5](https://medium.com/analytics-and-data/challenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1)

Photo by [Austin Distel](https://unsplash.com/@austindistel?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

Multiple organizational factors can impact how likely data science projects are to work out. Having teams empowered to put application code into production, the composition of the teams, and the organization’s mentality all contribute to the success of the project.

Empowerment
-----------

In large traditional companies, it is often the case that the data science team is not empowered to put models into productions. From not having access to production data, to not being allowed to push code or applications to…

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-and-data%2F458d89b6a9a1&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&user=Julien+Kervizic&userId=62925548a091&source=-----458d89b6a9a1---------------------clap_footer-----------)

\--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-and-data%2F458d89b6a9a1&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&user=Julien+Kervizic&userId=62925548a091&source=-----458d89b6a9a1---------------------clap_footer-----------)

\--

[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F458d89b6a9a1&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&source=--------------------------bookmark_footer-----------)

[![Image 6: Julien Kervizic](https://miro.medium.com/v2/resize:fill:144:144/0*8KNHDwr8y_G86xGi.)](https://medium.com/@julienkervizic?source=post_page-----458d89b6a9a1--------------------------------)[![Image 7: Hacking Analytics](https://miro.medium.com/v2/resize:fill:64:64/1*rwY9i9rSi9ySOlgw_ZO7yQ.png)](https://medium.com/analytics-and-data?source=post_page-----458d89b6a9a1--------------------------------)

Follow

[](https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F624087227afa&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&newsletterV3=62925548a091&newsletterV3Id=624087227afa&user=Julien+Kervizic&userId=62925548a091&source=-----458d89b6a9a1---------------------subscribe_user-----------)

[Written by Julien Kervizic --------------------------](https://medium.com/@julienkervizic?source=post_page-----458d89b6a9a1--------------------------------)

[2.9K Followers](https://medium.com/@julienkervizic/followers?source=post_page-----458d89b6a9a1--------------------------------)

·Editor for

[Hacking Analytics](https://medium.com/analytics-and-data?source=post_page-----458d89b6a9a1--------------------------------)

Living at the interstice of business, data and technology | Head of Data at iptiQ by SwissRe | previously at Facebook, Amazon | [julienkervizic@gmail.com](mailto:julienkervizic@gmail.com)

Follow

[](https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F624087227afa&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-and-data%2Fchallenges-moving-data-science-proof-of-concept-to-production-458d89b6a9a1&newsletterV3=62925548a091&newsletterV3Id=624087227afa&user=Julien+Kervizic&userId=62925548a091&source=-----458d89b6a9a1---------------------subscribe_user-----------)

[Help](https://help.medium.com/hc/en-us?source=post_page-----458d89b6a9a1--------------------------------)

[Status](https://medium.statuspage.io/?source=post_page-----458d89b6a9a1--------------------------------)

[About](https://medium.com/about?autoplay=1&source=post_page-----458d89b6a9a1--------------------------------)

[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----458d89b6a9a1--------------------------------)

[Press](https://medium.com/analytics-and-data/pressinquiries@medium.com?source=post_page-----458d89b6a9a1--------------------------------)

[Blog](https://blog.medium.com/?source=post_page-----458d89b6a9a1--------------------------------)

[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----458d89b6a9a1--------------------------------)

[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----458d89b6a9a1--------------------------------)

[Text to speech](https://speechify.com/medium?source=post_page-----458d89b6a9a1--------------------------------)

[Teams](https://medium.com/business?source=post_page-----458d89b6a9a1--------------------------------)


[3] Title: What Is Retrieval-Augmented Generation aka RAG?
[3] URL Source: https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/
[3] Description: Retrieval-augmented generation (<strong>RAG</strong>) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.
[3] Published Time: 2023-11-15T16:00:25+00:00
[3] Markdown Content:
_Editor’s note: This article was updated on September 23, 2024._

To understand the latest advance in [generative AI](https://www.nvidia.com/en-us/glossary/data-science/generative-ai/), imagine a courtroom.

Judges hear and decide cases based on their general understanding of the law. Sometimes a case — like a malpractice suit or a labor dispute — requires special expertise, so judges send court clerks to a law library, looking for precedents and specific cases they can cite.

Like a good judge, large language models ([LLMs](https://www.nvidia.com/en-us/glossary/data-science/large-language-models/)) can respond to a wide variety of human queries. But to deliver authoritative answers that cite sources, the model needs an assistant to do some research.

The court clerk of AI is a process called [retrieval-augmented generation](https://www.nvidia.com/en-us/glossary/retrieval-augmented-generation/), or RAG for short.

**How It Got Named ‘RAG’**
--------------------------

Patrick Lewis, lead author of the [2020 paper that coined the term](https://arxiv.org/pdf/2005.11401.pdf), apologized for the unflattering acronym that now describes a growing family of methods across hundreds of papers and dozens of commercial services he believes represent the future of generative AI.

[![Image 1: Picture of Patrick Lewis, lead author of RAG paper](https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author-150x150.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author.jpg)

Patrick Lewis

“We definitely would have put more thought into the name had we known our work would become so widespread,” Lewis said in an interview from Singapore, where he was sharing his ideas with a regional conference of database developers.

“We always planned to have a nicer sounding name, but when it came time to write the paper, no one had a better idea,” said Lewis, who now leads a RAG team at AI startup Cohere.

Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.

In other words, it fills a gap in how LLMs work. Under the hood, LLMs are neural networks, typically measured by how many parameters they contain. An LLM’s parameters essentially represent the general patterns of how humans use words to form sentences.

That deep understanding, sometimes called parameterized knowledge, makes LLMs useful in responding to general prompts at light speed. However, it does not serve users who want a deeper dive into a current or more specific topic.

**Combining Internal, External Resources**
------------------------------------------

Lewis and colleagues developed retrieval-augmented generation to link generative AI services to external resources, especially ones rich in the latest technical details.

The paper, with coauthors from the former Facebook AI Research (now Meta AI), University College London and New York University, called RAG “a general-purpose fine-tuning recipe” because it can be used by nearly any LLM to connect with practically any external resource.

**Building User Trust**
-----------------------

Retrieval-augmented generation gives models sources they can cite, like footnotes in a research paper, so users can check any claims. That builds trust.

What’s more, the technique can help models clear up ambiguity in a user query. It also reduces the possibility a model will make a wrong guess, a phenomenon sometimes called hallucination.

Another great advantage of RAG is it’s relatively easy. A [blog](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) by Lewis and three of the paper’s coauthors said developers can implement the process with as few as [five lines of code](https://huggingface.co/facebook/rag-token-nq).

That makes the method faster and less expensive than retraining a model with additional datasets. And it lets users hot-swap new sources on the fly.

**How People Are Using RAG**
----------------------------

With retrieval-augmented generation, users can essentially have conversations with data repositories, opening up new kinds of experiences. This means the applications for RAG could be multiple times the number of available datasets.

For example, a generative AI model supplemented with a medical index could be a great assistant for a doctor or nurse. Financial analysts would benefit from an assistant linked to market data.

In fact, almost any business can turn its technical or policy manuals, videos or logs into resources called knowledge bases that can enhance LLMs. These sources can enable use cases such as customer or field support, employee training and developer productivity.

The broad potential is why companies including [AWS](https://aws.amazon.com/blogs/machine-learning/simplify-access-to-internal-information-using-retrieval-augmented-generation-and-langchain-agents/), [IBM](https://research.ibm.com/blog/retrieval-augmented-generation-RAG), [Glean](https://www.glean.com/), Google, Microsoft, NVIDIA, [Oracle](https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/) and [Pinecone](https://www.pinecone.io/learn/retrieval-augmented-generation/) are adopting RAG.

**Getting Started With Retrieval-Augmented Generation** 
--------------------------------------------------------

To help users get started, NVIDIA developed an [AI workflow for retrieval-augmented generation](https://www.nvidia.com/en-us/ai-data-science/ai-workflows/generative-ai-chatbots/). It includes a sample chatbot and the elements users need to create their own applications with this new method.

The workflow uses [NVIDIA NeMo Retriever](https://www.nvidia.com/en-us/ai-data-science/products/nemo/), a collection of easy-to-use [NVIDIA NIM](https://www.nvidia.com/en-us/ai/) microservices for large scale information retrieval. NIM eases deployment of secure, high performance AI model inferencing across clouds, data centers and workstations.

These components are all part of [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/), a software platform that accelerates development and deployment of production-ready AI with the security, support and stability businesses need.

Getting the best performance for RAG workflows requires massive amounts of memory and compute to move and process data. The [NVIDIA GH200 Grace Hopper Superchip](https://nvidianews.nvidia.com/news/gh200-grace-hopper-superchip-with-hbm3e-memory), with its 288GB of fast HBM3e memory and 8 petaflops of compute, is ideal — it can deliver a 150x speedup over using a CPU.

Once companies get familiar with RAG, they can combine a variety of off-the-shelf or custom LLMs with internal or external knowledge bases to create a wide range of assistants that help their employees and customers.

RAG doesn’t require a data center. LLMs are debuting on Windows PCs, thanks to NVIDIA software that enables all sorts of applications users can access even on their laptops.

[![Image 2: Chart shows running RAG on a PC](https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg)

An example application for RAG on a PC.

PCs equipped with NVIDIA RTX GPUs can now run some AI models locally. By using RAG on a PC, users can link to a private knowledge source – whether that be emails, notes or articles – to improve responses. The user can then feel confident that their data source, prompts and response all remain private and secure.

A [recent blog](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fblogs.nvidia.com%2Fblog%2F2023%2F10%2F17%2Ftensorrt-llm-windows-stable-diffusion-rtx%2F&data=05%7C01%7Crmerritt%40nvidia.com%7Cdfd2267cb8344597f73408dbd0e6fc36%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638333462716560839%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=V5FClNyBybzOnm3%2FKxiTT4i9aoT0GdLWzdLfnF8LBk0%3D&reserved=0) provides an example of RAG accelerated by TensorRT-LLM for Windows to get better results fast.

**The History of RAG** 
-----------------------

The roots of the technique go back at least to the early 1970s. That’s when researchers in information retrieval prototyped what they called question-answering systems, apps that use natural language processing ([NLP](https://www.nvidia.com/en-us/glossary/natural-language-processing/)) to access text, initially in narrow topics such as baseball.

The concepts behind this kind of text mining have remained fairly constant over the years. But the machine learning engines driving them have grown significantly, increasing their usefulness and popularity.

In the mid-1990s, the Ask Jeeves service, now Ask.com, popularized question answering with its mascot of a well-dressed valet. IBM’s Watson became a TV celebrity in 2011 when it handily beat two human champions on the _Jeopardy!_ game show.

[![Image 3: Picture of Ask Jeeves, an early RAG-like web service](https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2.jpg)

Today, LLMs are taking question-answering systems to a whole new level.

**Insights From a London Lab**
------------------------------

The seminal 2020 paper arrived as Lewis was pursuing a doctorate in NLP at University College London and working for Meta at a new London AI lab. The team was searching for ways to pack more knowledge into an LLM’s parameters and using a benchmark it developed to measure its progress.

Building on earlier methods and inspired by [a paper](https://arxiv.org/pdf/2002.08909.pdf) from Google researchers, the group “had this compelling vision of a trained system that had a retrieval index in the middle of it, so it could learn and generate any text output you wanted,” Lewis recalled.

[![Image 4: Picture of IBM Watson winning on "Jeopardy" TV show, popularizing a RAG-like AI service](https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT.jpg)

The IBM Watson question-answering system became a celebrity when it won big on the TV game show Jeopardy!

When Lewis plugged into the work in progress a promising retrieval system from another Meta team, the first results were unexpectedly impressive.

“I showed my supervisor and he said, ‘Whoa, take the win. This sort of thing doesn’t happen very often,’ because these workflows can be hard to set up correctly the first time,” he said.

Lewis also credits major contributions from team members Ethan Perez and Douwe Kiela, then of New York University and Facebook AI Research, respectively.

When complete, the work, which ran on a cluster of NVIDIA GPUs, showed how to make generative AI models more authoritative and [trustworthy](https://blogs.nvidia.com/blog/what-is-trustworthy-ai/). It’s since been cited by hundreds of papers that amplified and extended the concepts in what continues to be an active area of research.

**How Retrieval-Augmented Generation Works**
--------------------------------------------

At a high level, here’s how an [NVIDIA technical brief](https://docs.nvidia.com/ai-enterprise/workflows-generative-ai/0.1.0/technical-brief.html) describes the RAG process.

When users ask an LLM a question, the AI model sends the query to another model that converts it into a numeric format so machines can read it. The numeric version of the query is sometimes called an embedding or a vector.

[![Image 5: NVIDIA diagram of how RAG works with LLMs](https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg)

Retrieval-augmented generation combines LLMs with embedding models and [vector databases](https://www.nvidia.com/en-us/glossary/vector-database/).

The embedding model then compares these numeric values to vectors in a machine-readable index of an available knowledge base. When it finds a match or multiple matches, it retrieves the related data, converts it to human-readable words and passes it back to the LLM.

Finally, the LLM combines the retrieved words and its own response to the query into a final answer it presents to the user, potentially citing sources the embedding model found.

**Keeping Sources Current**
---------------------------

In the background, the embedding model continuously creates and updates machine-readable indices, sometimes called vector databases, for new and updated knowledge bases as they become available.

[![Image 6: Chart of a RAG process described by LangChain](https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-672x268.jpg)](https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process.jpg)

Retrieval-augmented generation combines LLMs with embedding models and vector databases.

Many developers find LangChain, an open-source library, can be particularly useful in chaining together LLMs, embedding models and knowledge bases. NVIDIA uses LangChain in its reference architecture for retrieval-augmented generation.

The LangChain community provides its own [description of a RAG process](https://blog.langchain.dev/tutorial-chatgpt-over-your-data/).

Looking forward, the future of generative AI lies in creatively chaining all sorts of LLMs and knowledge bases together to create new kinds of assistants that deliver authoritative results users can verify.

Get a hands on using retrieval-augmented generation with an AI chatbot in this [NVIDIA LaunchPad lab](https://www.nvidia.com/en-us/launchpad/ai/generative-ai-knowledge-base-chatbot/).

_Explore [generative AI](https://www.nvidia.com/gtc/sessions/generative-ai/?nvid=nv-int-txtad-141445 "Original URL: https://www.nvidia.com/gtc/sessions/generative-ai/?nvid=nv-int-txtad-141445 Click to follow link.") sessions and experiences at [NVIDIA GTC](https://www.nvidia.com/gtc/), the global conference on AI and accelerated computing, running March 18-21 in San Jose, Calif., and online._


[4] Title: Retrieval-Augmented Generation (RAG) from basics to advanced
[4] URL Source: https://medium.com/@tejpal.abhyuday/retrieval-augmented-generation-rag-from-basics-to-advanced-a2b068fd576c
[4] Description: In the context of Retrieval-Augmented Generation (<strong>RAG</strong>) in Large Language Models (LLMs), here are the advantages and disadvantages of the three approaches:
[4] Published Time: 2024-02-14T09:26:21.895Z
[4] Markdown Content:
[![Image 1: Tejpal Kumawat](https://miro.medium.com/v2/resize:fill:88:88/1*SO4ANe0jlgNeT35lvyq2uQ.png)](https://medium.com/@tejpal.abhyuday?source=post_page-----a2b068fd576c--------------------------------)

Introduction:
-------------

*   Retrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge.
*   This is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.

Challenge:
----------

*   Clients often have vast proprietary documents.
*   Extracting specific information is like finding a needle in a haystack.

2\. GPT4-Turbo Introduction:
----------------------------

*   OpenAI’s GPT4-Turbo can process large documents.

3\. Efficiency Issue:
---------------------

*   “Lost In The Middle” phenomenon hampers efficiency.
*   Model forgets content in the middle of its contextual window.

4\. Alternative Approach — Retrieval-Augmented-Generation (RAG):
----------------------------------------------------------------

*   Create an index for each document paragraph.
*   Swiftly identify pertinent paragraphs.
*   Feed selected paragraphs into a Large Language Model (LLM) like GPT4.

5\. Advantages:
---------------

*   Prevents information overload.
*   Enhances result quality by providing only relevant paragraphs.

The Retrieval Augmented Generation (RAG) Pipeline
-------------------------------------------------

*   With RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases.
*   It leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.
*   The retriever here could be any of the following depending on the need for semantic retrieval or not:
*   **Vector database:** Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.
*   **Graph database:** Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.
*   **Regular SQL database:** Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.
*   The image below from [Damien Benveniste, PhD](https://www.linkedin.com/posts/damienbenveniste_machinelearning-datascience-artificialintelligence-activity-7119708674868051969-5HA1?utm_source=share&utm_medium=member_desktop) talks a bit about the difference between using Graph vs Vector database for RAG.

*   Graph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data.
*   Graph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.
*   A potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.
*   After retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.
*   Let’s succinctly summarize the process of RAG and then delve into its pros and cons:

1.  **Vector Database Creation**: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).
2.  **User Input**: A user provides a query in natural language, seeking an answer or completion.
3.  **Information Retrieval**: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user’s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.
4.  **Combining Data**: The chosen data segments from the database are combined with the user’s initial query, creating an expanded prompt.
5.  **Generating Text**: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.

*   The image below [(source)](https://vectara.com/retrieval-augmented-generation-everything-you-need-to-know/) displays the high-level working of RAG.

Difference Between RAG and Gine Tuning of the LLM :

1.  Retrieval systems (RAG) give LLM systems access to factual, access-controlled, timely information. Fine tuning _can not do this_, so there’s no competition.
2.  Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style
3.  All in all, focus on RAG first. A successful LLM application _must_ connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.

Choice of the Vector Database :
-------------------------------

Building a RAG Pipeline
-----------------------

*   The image below [(source)](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/2/advanced-rag-pipeline), gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.

*   In the sections below, we will go over these key areas.

Ingestion
---------

Chunking
--------

*   Chunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences or paragraphs.
*   In RAG, each chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user’s query and the content, enhancing the accuracy and relevance of the information retrieved.
*   Larger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.
*   So the next natural question that comes up is, how do you choose the right chunk size for your use case? The choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context’s integrity. Let’s look at a few methods below referred from [Pinecone](https://www.pinecone.io/learn/chunking-strategies/):
*   **Fixed-size chunking:** Simply decide the number of tokens in our chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.

text = "..." # your text  
from langchain.text\_splitter import CharacterTextSplitter  
text\_splitter = CharacterTextSplitter(  
    separator = "\\n\\n",  
    chunk\_size = 256,  
    chunk\_overlap  = 20  
)  
docs = text\_splitter.create\_documents(\[text\])

*   **Context-aware chunking:** Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:

1.  Sentence Splitting This method aligns with models optimized for embedding sentence-level content. Different tools and techniques can be used for sentence splitting:

*   **Naive Splitting:** A basic method where sentences are split using periods and new lines. Example:

  text = "..."  # Your text  
   docs = text.split(".")

*   **NLTK (Natural Language Toolkit):** A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:

text = "..."  # Your text  
from langchain.text\_splitter import NLTKTextSplitter  
text\_splitter = NLTKTextSplitter()  
docs = text\_splitter.split\_text(text)

*   **spaCy:** An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:

text = "..."  # Your text  
from langchain.text\_splitter import SpacyTextSplitter  
text\_splitter = SpacyTextSplitter()  
docs = text\_splitter.split\_text(text)

1.  **Recursive Chunking:** Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:

   text = "..."  # Your text  
   from langchain.text\_splitter import RecursiveCharacterTextSplitter  
   text\_splitter = RecursiveCharacterTextSplitter(  
       chunk\_size = 256,  
       chunk\_overlap = 20  
   )  
   docs = text\_splitter.create\_documents(\[text\])

*   “As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.” [(source)](https://www.pinecone.io/learn/chunking-strategies/)

Embeddings
----------

*   Once you have your prompt chunked appropriately, the next step is to embedd it. Embedding prompts and documents in RAG involves transforming both the user’s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG’s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here’s how it typically works:
*   One option to help pick which embedding model would be best suited for your task is to look at [HuggingFace’s Massive Text Embedding Benchmark (MTEB) leaderboard.](https://huggingface.co/spaces/mteb/leaderboard). There is a question of whether a dense or sparse embedding can be used so let’s look into benefits of each below:
*   **Sparse embedding:** Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It’s computationally less intensive but may not capture the deeper semantic meanings in the text.
*   **Semantic embedding:** Semantic embeddings, such as BERT or SentenceBERT lend themselves naturally to the RAG usecase.
*   **BERT:** Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.
*   **SentenceBERT:** Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG

Retrieval
---------

*   Let’s look at two different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.

Standard/Naive Approach
-----------------------

*   As we see in the image below [(source)](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/2/advanced-rag-pipeline), the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.

In the context of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs), here are the advantages and disadvantages of the three approaches:

Advantages

1.  **Simplicity and Efficiency**: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.
2.  **Uniformity in Data Handling**: It maintains consistency in the data used across both retrieval and synthesis phases.

Disadvantages

1.  **Limited Contextual Understanding**: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.
2.  **Potential for Suboptimal Responses**: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.

Sentence-Window Retrieval / Small-to-Large Chunking
---------------------------------------------------

*   The sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences.
*   It decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis it adds back in the context around the retrieved chunks, as seen in the image below [(source)](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/2/advanced-rag-pipeline).

*   During retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved) as shown in the figure below [(source)](https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/2/advanced-rag-pipeline).

Advantages

1.  **Enhanced Specificity in Retrieval**: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.
2.  **Context-Rich Synthesis**: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.
3.  **Balanced Approach**: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.

Disadvantages

1.  **Increased Complexity**: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.
2.  **Potential Contextual Gaps**: There’s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.

Retriever Ensembling and Reranking
----------------------------------

*   Thought: what if we could try a bunch of chunk sizes at once, and have a re-ranker prune the results?
*   This achieves two purposes:
*   Better (albeit more costly) retrieved results by pooling results from multiple chunk sizes, assuming the re-ranker has a reasonable level of performance.
*   A way to benchmark different retrieval strategies against each other (w.r.t. the re-ranker).
*   The process is as follows:
*   Chunk up the same document in a bunch of different ways, say with chunk sizes: 128, 256, 512, and 1024.
*   During retrieval, we fetch relevant chunks from each retriever, thus ensembling them together for retrieval.
*   Use a re-ranker to rank/prune results.
*   The following figure [(source)](https://www.linkedin.com/posts/llamaindex_a-big-pain-point-when-trying-to-build-rag-activity-7113307664616484864-RT6n/) delineates the process.

*   Based on [evaluation results from LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/retrievers/ensemble_retrieval.html), faithfulness metrics go up slightly for the ensembled approach, indicating retrieved results are slightly more relevant. But pairwise comparisons lead to equal preference for both approaches, making it still questionable as to whether or not ensembling is better.
*   Note that the ensembling strategy can be applied for other aspects of a RAG pipeline too, beyond chunk size, such as vector vs. keyword vs. hybrid search, etc.

Re-ranking
----------

*   Re-ranking in RAG refers to the process of evaluating and sorting the retrieved documents or information snippets based on their relevance to the given query or task.
*   There are different types of re-ranking techniques used in RAG:
*   **Lexical Re-Ranking:** This involves re-ranking based on lexical similarity between the query and the retrieved documents. Methods like BM25 or cosine similarity with TF-IDF vectors are common.
*   **Semantic Re-Ranking:** This type of re-ranking uses semantic understanding to judge the relevance of documents. It often involves neural models like BERT or other transformer-based models to understand the context and meaning beyond mere word overlap.
*   **Learning-to-Rank (LTR) Methods:** These involve training a model specifically for the task of ranking documents (point-wise, pair-wise, and list-wise) based on features extracted from both the query and the documents. This can include a mix of lexical, semantic, and other features.
*   **Hybrid Methods:** These combine lexical and semantic approaches, possibly with other signals like user feedback or domain-specific features, to improve re-ranking.
*   Neural LTR methods are most commonly used at this stage since the candidate set is limited to dozens of samples. Some common neural models used for re-ranking are:
*   [Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) (monoBERT and duo BERT)
*   [Pretrained Transformers for Text Ranking BERT and Beyond](https://arxiv.org/abs/2010.06467)
*   [ListT5](https://arxiv.org/abs/2312.16098)
*   [ListBERT](https://arxiv.org/abs/2206.15198)

Response Generation / Synthesis
-------------------------------

*   The last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user’s original query, maintaining a natural and conversational tone.
*   Note that while creating the expanded prompt (with the retrieved top-k� chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system’s effectiveness and thus make the system more performant. This is summarized in the paper below.

[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2302.12345)
---------------------------------------------------------------------------------------------

*   While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context.
*   This paper by Liu et al. from Percy Liang’s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.
*   They tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.
*   They find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.
*   A summary of their learnings is as follows:
*   Best performance when the relevant information is at the beginning.
*   Performance decreases with an increase in context length.
*   Too many retrieved documents harm performance.
*   Improving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.
*   Extended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.

Conclusion:

We discussed about the RAG working , reranking , and so many concepts. I hope you find it useful.

Thanks

Reference:


[5] Title: What is Retrieval-Augmented Generation(RAG) in LLM and How it works?
[5] URL Source: https://medium.com/@sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172
[5] Description: In the fast-paced world of artificial intelligence, language models have undergone a remarkable evolution. From the early days of simple rule-based systems to the sophisticated neural networks we see…
[5] Published Time: 2024-04-22T13:04:48.866Z
[5] Markdown Content:
[![Image 1: Sahin Ahmed, Data Scientist](https://miro.medium.com/v2/resize:fill:88:88/1*AgUnbJnTepkcw5CX6T_EEA.jpeg)](https://medium.com/@sahin.samia?source=post_page-----a8c79e35a172--------------------------------)

In the fast-paced world of artificial intelligence, language models have undergone a remarkable evolution. From the early days of simple rule-based systems to the sophisticated neural networks we see today, each step has significantly expanded what AI can do with language. A pivotal development in this journey is the introduction of Retrieval-Augmented Generation, or RAG.

RAG represents a blend of traditional language models with an innovative twist: it integrates information retrieval directly into the generation process. Think of it as having an AI that can look up information in a library of texts before responding, making it more knowledgeable and context-aware. This capability is not just an improvement — it’s a game changer. It allows models to produce responses that are not only accurate but also deeply informed by relevant, real-world information.

What is Retrieval-Augmented Generation (RAG)?
---------------------------------------------

In traditional language models, responses are generated based solely on pre-learned patterns and information during the training phase. However, these models are inherently limited by the data they were trained on, often leading to responses that might lack depth or specific knowledge. RAG addresses this limitation by pulling in external data as needed during the generation process. Here’s how it works: **when a query is made, the RAG system first retrieves relevant information from a large dataset or knowledge base, then this information is used to inform and guide the generation of the response.**

The RAG architecture
--------------------

It is a sophisticated system designed to enhance the capabilities of large language models by combining them with powerful retrieval mechanisms. It’s essentially a two-part process involving a retriever component and a generator component. Let’s break down each component and their roles in the overall process:

Image Source:[https://snorkel.ai/which-is-better-retrieval-augmentation-rag-or-fine-tuning-both/](https://snorkel.ai/which-is-better-retrieval-augmentation-rag-or-fine-tuning-both/)

Retriever Component:
--------------------

*   Function: The retriever’s job is to find relevant documents or pieces of information that can help answer a query. It takes the input query and searches a database to retrieve information that might be useful for generating a response.

**Types of Retrievers:**

*   **Dense Retrievers:** These use neural network-based methods to create dense vector embeddings of the text. They tend to perform better when the meaning of the text is more important than the exact wording since the embeddings capture semantic similarities.
*   **Sparse Retrievers:** These rely on term-matching techniques like TF-IDF or BM25. They excel at finding documents with exact keyword matches, which can be particularly useful when the query contains unique or rare terms.

Generator Component:
--------------------

*   **Function:** The generator is a language model that produces the final text output. It takes the input query and the contexts retrieved by the retriever to generate a coherent and relevant response.
*   **Interaction with Retriever:** The generator doesn’t work in isolation; it uses the context provided by the retriever to inform its response, ensuring that the output is not just plausible, but also rich in detail and accuracy.

The workflow of a Retrieval-Augmented Generation (RAG) system
-------------------------------------------------------------

Image Source:[https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)

1.  **Query Processing:** It all starts with a query. This could be a question, a prompt, or any input that you want the language model to respond to.
2.  **Embedding Model:** The query is then passed to an embedding model. This model converts the query into a vector, which is a numerical representation that can be understood and processed by the system.
3.  **Vector Database (DB) Retrieval:** The query vector is used to search through a vector database. This database contains precomputed vectors of potential contexts that the model can use to generate a response. The system retrieves the most relevant contexts based on how closely their vectors match the query vector.
4.  **Retrieved Contexts:** The contexts that have been retrieved are then passed along to the Large Language Model (LLM). These contexts contain the information that the LLM uses to generate a knowledgeable and accurate response.
5.  **LLM Response Generation:** The LLM takes into account both the original query and the retrieved contexts to generate a comprehensive and relevant response. It synthesizes the information from the contexts to ensure that the response is not only based on its pre-existing knowledge but is also augmented with specific details from the retrieved data.
6.  **Final Response:** Finally, the LLM outputs the response, which is now informed by the external data retrieved in the process, making it more accurate and detailed.

**Choosing a Retriever:** The choice between dense and sparse retrievers often depends on the nature of the database and the types of queries expected. Dense retrievers are more computationally intensive but can capture deep semantic relationships, while sparse retrievers are faster and better for specific term matches.

**Hybrid Models:** Some RAG systems may use hybrid retrievers that combine dense and sparse techniques to balance the trade-offs and take advantage of both methods

Applications of RAG:
--------------------

Retrieval-Augmented Generation (RAG) finds applications in numerous areas within the AI landscape, significantly enhancing the quality and relevance of the outputs generated by language models.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Enhancing Chatbots and Conversational Agents:
---------------------------------------------

*   **Customer Support:** Chatbots equipped with RAG can retrieve product information, FAQs, and support documents to provide detailed and accurate responses to customer inquiries.
*   **Personal Assistants:** Virtual personal assistants use RAG to pull in real-time data, such as weather information or news, making their interactions more contextually relevant and helpful.

Improving Accuracy and Depth in Automated Content Generation:
-------------------------------------------------------------

*   **Content Creation:** Journalistic AI tools use RAG to fetch relevant facts and figures, leading to articles that are rich with up-to-date information and require less human editing.
*   **Copywriting:** Marketing bots utilize RAG to generate product descriptions and advertising copy that are not only creative but also factually correct, by referencing a database of product specs and reviews.

Application in Question-Answering Systems:
------------------------------------------

*   **Educational Platforms:** RAG is used in educational technology to provide students with detailed explanations and additional context for complex subjects by retrieving information from educational databases.
*   **Research:** AI systems help researchers find answers to scientific questions by referencing a vast corpus of academic papers and generating summaries of relevant studies.

Benefits of Using RAG in Various Fields:
----------------------------------------

*   **Healthcare:** RAG-powered systems can assist medical professionals by pulling in information from medical journals and patient records to suggest diagnoses or treatments that are informed by the latest research.
*   **Customer Service:** By retrieving company policies and customer histories, RAG allows service agents to offer personalized and accurate advice, improving customer satisfaction.
*   **Education:** Teachers can leverage RAG-based tools to create custom lesson plans and learning materials that draw from a broad range of educational content, providing students with diverse perspectives.

Additional Applications:
------------------------

*   **Legal Aid:** RAG systems can aid in legal research by fetching relevant case law and statutes to assist in drafting legal documents or preparing for cases.
*   **Translation Services:** Combining RAG with translation models to provide context-aware translations that consider cultural nuances and idiomatic expressions by referencing bilingual text corpora.

The utilization of RAG in these applications allows for outputs that are not just generated based on a static knowledge base but are dynamically informed by the most relevant and current data available, leading to more precise, informative, and trustworthy AI-generated content.

Challenges in Implementing RAG:
-------------------------------

*   **Complexity:** Combining retrieval and generation processes adds complexity to the model architecture, making it more challenging to develop and maintain.
*   **Scalability:** Managing and searching through large databases efficiently is difficult, especially as the size and number of documents grow.
*   **Latency:** Retrieval processes can introduce latency, impacting the response time of the system which is critical for applications requiring real-time interactions, like conversational agents.
*   **Synchronization:** Keeping the retrieval database up-to-date with the latest information requires a synchronization mechanism that can handle constant updates without degrading performance.

Limitations of Current RAG Models:
----------------------------------

*   **Context Limitation:** RAG models may struggle when the context required to generate a response exceeds the size limitations of the model’s input window.
*   **Retrieval Errors:** The quality of the generated response is heavily dependent on the quality of the retrieval step; if irrelevant information is retrieved, the generation will suffer.
*   **Bias:** RAG models can inadvertently propagate and even amplify biases present in the data sources they retrieve information from.

Potential Areas for Improvement:
--------------------------------

*   **Better Integration:** Smoother integration of the retrieval and generation components could lead to improvements in the model’s ability to handle complex queries.
*   **Enhanced Retrieval Algorithms:** More sophisticated retrieval algorithms could provide more accurate and relevant context, improving the overall quality of the generated content.
*   **Adaptive Learning:** Incorporating mechanisms that allow the model to learn from its retrieval successes and failures can refine the system over time.

Data Dependency and Retrieval Sources:
--------------------------------------

*   **Data Quality:** The effectiveness of a RAG system is directly tied to the quality of the data in the retrieval database. Poor quality or outdated information can lead to incorrect outputs.
*   **Source Reliability:** It’s critical to ensure that the sources of information are reliable and authoritative, especially for applications like healthcare and education.
*   **Privacy and Security:** When dealing with sensitive information, such as personal data or proprietary content, there are significant concerns around data privacy and security.

Emerging Trends and Ongoing Research:
-------------------------------------

*   **Cross-modal Retrieval:** Expanding RAG capabilities to retrieve not only textual information but also data from other modalities like images and videos, enabling richer multi-modal responses.
*   **Continuous Learning:** Developing RAG systems that learn from each interaction, thus improving their retrieval and generation capabilities over time without the need for retraining.
*   **Interactive Retrieval:** Enhancing the retrieval process to be more interactive, allowing the generator to ask for more information or clarification, much like a human would in a dialogue.
*   **Domain Adaptation:** Tailoring RAG models for specific domains, such as legal or medical, to improve the relevance and accuracy of information retrieval.

Potential Future Enhancements:
------------------------------

*   **Personalization:** Integrating user profiles and historical interactions to personalize responses, making RAG models more effective in customer service and recommendation systems.
*   **Knowledge Grounding:** Using external knowledge bases not just for retrieval but also for grounding the responses in verifiable facts, which is crucial for educational and informational applications.
*   **Efficient Indexing:** Employing more efficient data structures and algorithms for indexing the database to speed up retrieval and reduce computational costs.
